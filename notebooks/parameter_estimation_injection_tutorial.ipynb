{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "OjUNeFsxyguu"
   },
   "source": [
    "# Gravitational Wave Parameter-Estimation Workshop\n",
    "\n",
    "## Tutorial: Parameter estimation for a GW150914-like injection in simulated data\n",
    "\n",
    "(Note, this tutorial and the open-data tutorial (tutorial A) are based on the [GWOSC ODW #3 tutorial 2.4](https://github.com/gw-odw/odw-2020). You may wish to browse that repository for similar notebooks covering a range of gravitational-wave data analysis subjects). \n",
    "\n",
    "This example estimates the non-spinning parameters of an injected binary black hole system using\n",
    "commonly used prior distributions. This will take about 40 minutes to run.\n",
    "\n",
    "We shall also use the sampler `nestle` for the purposes of this tutorial.\n",
    "   \n",
    "More examples at https://lscsoft.docs.ligo.org/bilby/examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "VwsIdKJ3yguv"
   },
   "source": [
    "##  Installation  (execute only if running on a cloud platform!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "eJeo4XrHyguw",
    "outputId": "fc1a59d8-8ad9-494f-e7a1-9ac9b0775a9f"
   },
   "outputs": [],
   "source": [
    "# -- Use the following line in Google Colab\n",
    "#! pip install -q 'lalsuite' 'bilby' 'gwpy' 'nestle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "f0q3Y_9gygu0"
   },
   "source": [
    "**Important:** With Google Colab, you may need to restart the runtime after running the cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "XK8fHu13ygu1"
   },
   "source": [
    "## Initialization\n",
    "\n",
    "We begin by importing some commonly used functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "HyRSGt6cygu2"
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import bilby\n",
    "from bilby.core.prior import Uniform\n",
    "from bilby.gw.conversion import convert_to_lal_binary_black_hole_parameters, generate_all_bbh_parameters\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bilby version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bilby.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "_Hd4d4KVygu6"
   },
   "source": [
    "## Creating fake data\n",
    "In this notebook, we'll analyse GW150914-like injection. Our first task is to create some data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a random seed for result reproducibility.  This is optional!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "1cUhLaFIygu6"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to specify the parameters of our injection. You can set this up as a python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injection_parameters = dict(\n",
    "    mass_1=36., mass_2=29., a_1=0.4, a_2=0.3, tilt_1=0.5, tilt_2=1.0,\n",
    "    phi_12=1.7, phi_jl=0.3, luminosity_distance=2000., theta_jn=0.4, psi=2.659,\n",
    "    phase=1.3, geocent_time=1126259642.413, ra=1.375, dec=-1.2108)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to set up some additional arguments such as what waveform approximant to use and what the minimum frequencies for analysis are. This can again be set up as a python dictionary. While the duration and sampling frequenciy can be set up as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_arguments = dict(waveform_approximant='IMRPhenomPv2',\n",
    "                          reference_frequency=50., minimum_frequency=20., catch_waveform_errors=True)\n",
    "duration = 4.\n",
    "sampling_frequency = 2048."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now set up a bilby waveform generator. This wraps up some of the jobs of converting between parameters etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_generator = bilby.gw.WaveformGenerator(\n",
    "    duration=duration, sampling_frequency=sampling_frequency,\n",
    "    frequency_domain_source_model=bilby.gw.source.lal_binary_black_hole,\n",
    "    parameter_conversion=bilby.gw.conversion.convert_to_lal_binary_black_hole_parameters,\n",
    "    waveform_arguments=waveform_arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set up the bilby interferometers and inject data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifos = bilby.gw.detector.InterferometerList(['H1', 'L1'])\n",
    "ifos.set_strain_data_from_power_spectral_densities(\n",
    "    sampling_frequency=sampling_frequency, duration=duration,\n",
    "    start_time=injection_parameters['geocent_time'] - 3)\n",
    "injection = ifos.inject_signal(\n",
    "    waveform_generator=waveform_generator,\n",
    "    parameters=injection_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the data\n",
    "Okay, we have spent a bit of time now initializing things. Let's check that everything makes sense. To do this, we'll plot our analysis data alongwise the amplitude spectral density (ASD); this is just the square root of the PSD and has the right units to be comparable to the frequency-domain strain data. We also add the \"injection\", here we plot the absolute value of the (complex) plus polarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H1 = ifos[0]\n",
    "H1_injection = injection[0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "idxs = H1.strain_data.frequency_mask  # This is a boolean mask of the frequencies which we'll use in the analysis\n",
    "ax.loglog(H1.strain_data.frequency_array[idxs],\n",
    "          np.abs(H1.strain_data.frequency_domain_strain[idxs]),\n",
    "          label=\"data\")\n",
    "ax.loglog(H1.frequency_array[idxs],\n",
    "          H1.amplitude_spectral_density_array[idxs],\n",
    "          label=\"ASD\")\n",
    "ax.loglog(H1.frequency_array[idxs],\n",
    "          np.abs(H1_injection[\"plus\"][idxs]),\n",
    "          label=\"Abs. val. of plus polarization\")\n",
    "ax.set_xlabel(\"Frequency [Hz]\")\n",
    "ax.set_ylabel(\"Strain [strain/$\\sqrt{Hz}$]\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low dimensional analysis\n",
    "\n",
    "In general a compact binary coalescense signal is described by 15 parameters describing the masses, spins, orientation, and position of the two compact objects along with a time at which the signal merges. The goal of parameter estimation is to figure out what the data (and any cogent prior information) can tell us about the likely values of these parameters - this is called the \"posterior distribution of the parameters\".\n",
    "\n",
    "To start with, we'll analyse the data fixing all but a few of the parameters to known values (in Bayesian lingo - we use delta function priors), this will enable us to run things in a few minutes rather than the many hours needed to do full parameter estimation.\n",
    "\n",
    "We'll start by thinking about the mass of the system. We call the heavier black hole the primary and label its mass $m_1$ and that of the secondary (lighter) black hole $m_2$. In this way, we always define $m_1 \\ge m_2$. It turns out that inferences about $m_1$ and $m_2$ are highly correlated, we'll see exactly what this means later on.\n",
    "\n",
    "Bayesian inference methods are powerful at figuring out highly correlated posteriors. But, we can help it along by sampling in parameters which are not highly correlated. In particular, we define a new parameter called the [chirp mass](https://en.wikipedia.org/wiki/Chirp_mass) to be\n",
    "\n",
    "$$ \\mathcal{M} = \\frac{(m_1 m_2)^{3/5}}{(m_1 + m_2)^{1/5}} $$\n",
    "\n",
    "and the mass ratio\n",
    "\n",
    "$$ q = \\frac{m_{2}}{m_1} $$\n",
    "\n",
    "If we sample (make inferences about) $\\mathcal{M}$ and $q$, our code is much faster than if we use $m_1$ and $m_2$ directly! Note that so long as equivalent prior is given - one can also sample in the component masses themselves and you will get the same answer, it is just much slower!\n",
    "\n",
    "Once we have inferred $\\mathcal{M}$ and $q$, we can then derive $m_1$ and $m_2$ from the resulting samples (we'll do that in just a moment).\n",
    "\n",
    "Okay, let's run a short (~1min on a single 2.8GHz core), low-dimensional parameter estimation analysis. This is done by defining a prior dictionary where all parameters are fixed, except those that we want to vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a prior\n",
    "\n",
    "Here, we create a prior fixing everything except the chirp mass, mass ratio and geocent_time parameters to fixed values. The first two we described above. The geocent_time is the time at which it merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = bilby.core.prior.PriorDict()\n",
    "prior['chirp_mass'] = Uniform(name='chirp_mass', minimum=27.0,maximum=32.5)\n",
    "prior['mass_ratio'] = Uniform(name='mass_ratio', minimum=0.5, maximum=1)\n",
    "prior['geocent_time'] = Uniform(minimum=injection_parameters['geocent_time'] - 1,\n",
    "                                maximum=injection_parameters['geocent_time'] + 1,\n",
    "                                name='geocent_time', latex_label='$t_c$', unit='$s$')\n",
    "# We fix the rest of the parameters to their injected values\n",
    "for key in ['a_1', 'a_2', 'tilt_1', 'tilt_2', 'phi_12', 'phi_jl', 'psi', 'ra',\n",
    "            'dec','luminosity_distance', 'theta_jn', 'phase']:\n",
    "    prior[key] = injection_parameters[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a likelihood\n",
    "\n",
    "For Bayesian inference, we need to evaluate the likelihood. In Bilby, we create a likelihood object. This is the communication interface between the sampling part of Bilby and the data. Explicitly, when Bilby is sampling it only uses the `parameters` and `log_likelihood()` of the likelihood object. This means the likelihood can be arbitrarily complicated and the sampling part of Bilby won't mind a bit!\n",
    "\n",
    "Let's create a `GravitationalWaveTransient`, a special inbuilt method carefully designed to wrap up evaluating the likelihood of a waveform model in some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = bilby.gw.likelihood.GravitationalWaveTransient(\n",
    "    interferometers=ifos, waveform_generator=waveform_generator, priors=prior,\n",
    "    time_marginalization=True, phase_marginalization=True, distance_marginalization=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will print a warning about the `start_time`, it is safe to ignore this.\n",
    "\n",
    "Note that we also specify `time_marginalization=True` and `phase_marginalization=True`. This is a trick often used in Bayesian inference. We analytically marginalize (integrate) over the time/phase of the system while sampling, effectively reducing the parameter space and making it easier to sample. Bilby will then figure out (after the sampling) posteriors for these marginalized parameters. For an introduction to this topic, see [Thrane & Talbot (2019)](https://arxiv.org/abs/1809.02293)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "LCfygeVyygvM"
   },
   "source": [
    "Now that the prior is set-up and the likelihood is set-up (with the data and the signal mode), we can run the sampler to get the posterior result. This function takes the likelihood and prior along with some options for how to do the sampling and how to save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 627
    },
    "colab_type": "code",
    "id": "HHS9JSX3ygvN",
    "outputId": "69e07ce1-f118-4378-c750-5ec65d43e0db"
   },
   "outputs": [],
   "source": [
    "result_short = bilby.run_sampler(\n",
    "    likelihood, prior, sampler='nestle', outdir='short', label=\"GW150914\",\n",
    "    conversion_function=bilby.gw.conversion.generate_all_bbh_parameters,\n",
    "    sample=\"unif\", nlive=500, dlogz=3  # <- Arguments are used to make things fast - not recommended for general use\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "wKR045TIygvT"
   },
   "source": [
    "The `run_sampler` returned `result_short` - this is a Bilby result object. The posterior samples are stored in a [pandas data frame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) (think of this like a spreadsheet), let's take a look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_short.posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pull out specific parameters that we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_short.posterior[\"chirp_mass\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returned another `pandas` object. If you just want to get the numbers as a numpy array run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mc = result_short.posterior[\"chirp_mass\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then get some useful quantities such as the 90\\% credible interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = np.quantile(Mc, 0.05)\n",
    "upper_bound = np.quantile(Mc, 0.95)\n",
    "median = np.quantile(Mc, 0.5)\n",
    "print(\"Mc = {} with a 90% C.I = {} -> {}\".format(median, lower_bound, upper_bound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot the chirp mass in a histogram adding a region to indicate the 90\\% C.I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(result_short.posterior[\"chirp_mass\"], bins=20)\n",
    "ax.axvspan(lower_bound, upper_bound, color='C1', alpha=0.4)\n",
    "ax.axvline(median, color='C1')\n",
    "ax.set_xlabel(\"chirp mass\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result object also has in-built methods to make nice plots such as corner plots. You can add the priors if you are only plotting parameter which you sampled in, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_short.plot_corner(parameters=[\"chirp_mass\", \"mass_ratio\", \"geocent_time\"], prior=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also plot lines indicating specific points. Here, we add the values recorded on [GWOSC](https://www.gw-openscience.org/events/GW150914/). Notably, these fall outside the bulk of the posterior uncertainty here. This is because we limited our prior - if instead we ran the full analysis these agree nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "colab_type": "code",
    "id": "SB4AqmTaygvU",
    "outputId": "3b4d648b-378e-480f-93d8-7929c81f9559"
   },
   "outputs": [],
   "source": [
    "parameters = dict(mass_1=36, mass_2=29)\n",
    "result_short.plot_corner(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we discussed the \"correlation\" - in this plot we start to see the correlation between $m_1$ and $m_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta data\n",
    "The result object also stores meta data, like the priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_short.priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and details of the analysis itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_short.sampler_kwargs[\"npoints\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can also get out the Bayes factor for the signal vs. Gaussian noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ln Bayes factor = {} +/- {}\".format(\n",
    "    result_short.log_bayes_factor, result_short.log_evidence_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge questions\n",
    "First, let's take a closer look at the result obtained with the run above. What are the means of the chirp mass and mass ratio distributions? What are the medians of the distributions for the components masses? You can use `np.mean` and `np.median` to calculate these.\n",
    "\n",
    "Now let's expand on this example a bit. Rerun the analysis above but change the prior on the distance from a delta function to `bilby.core.prior.PowerLaw(alpha=2., minimum=50., maximum=800., name='luminosity_distance')`. You should also replace `sample='unif'` with `sample=\"rwalk\", nact=1, walks=1` in your call to `bilby.run_sampler` above. This will take a bit longer than the original run, around ~20 minutes. You also need to change the `label` in the call to `run_sampler` to avoid over-writing your results.\n",
    "\n",
    "What is the median reported value of the distance posterior? What is the new log Bayes factor for signal vs. Gaussian noise? Don't be alarmed if your results do not match the official LVC results, as these are not rigorous settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Tuto_2.4_Parameter_estimation_for_compact_object_mergers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
